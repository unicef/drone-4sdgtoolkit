![UNICEF logo blue with for every child motto.](/static/images/unicefinnovlogo.png)

# Use of Satellites or Drones for Imagery Collection

**Introduction**

Today, aerial imagery is available from a variety of sources, including traditional satellites, nano satellites, manned and unmanned aircrafts, balloons, and kites. This imagery can benefit humanitarian organisations in programming implementation, resource planning, community studies, and disaster response. Previously, the process of developing maps from imagery was painstakingly slow and it required a great deal of time from highly technically skilled individuals to patch different formatted images together.
This decision making guide aims to take the acquired knowledge from UNICEF’s use of satellites and drones for imagery collection and provide a guide on the different technologies as well as when to use a particular mode over the other. 

**Decision-making matrix**

The following tables provide a fact sheet on satellite and drone technology. When used together it will enable any technical or administrative decision maker to come to a quick and informed decision on which technology is best for any imagery needs. 

**Satellite imagery specifications;**

| Type | Sensor | Bands | Resolution | Coverage | Frequency |
| -- | -- | -- | -- | -- | -- |
| Commercial | Worldview3 (MAXAR) | Multi-spectral (8) | Pan-chromatic 	31 cm (pan) 1.24 m (multi spectral) | Global | Revisit time of <1 day collecting up to 680,000 km2 per day |
| Commercial | Planet scope | R,G,B, NIR | 3.7 m | Global | Revisit time < 1 day Collecting for the entire globe everyday |
| Commercial | ICEEYE | Synthetic Aperture Radar (SAR) | Up to 25cm | Global | 20 hours |
| Commercial | Capella Space | SAR | Up to 30 cm | Global | Maximum hourly | 	 
| Public | MODIS | Optical 36 bands | 250 m | Global | 1 day |  	 	 
| Public | VIIRS | Optical 21 and DNB (night-time light) | 375 m | Global | 1 day |
| Public | ALOS-2 PALSAR | SAR | 30 m | Global | 14 days |
| Public | Sentinel | Optical and SAR | 10 m | Global | 8 days |
| Public | Landsat | Optical | 30 m | Global | 16 days |

**Drone Imagery Specifications**

| Turnaround time | Resolution | Coverage | Products |
| --- | --- | --- | --- |
| 5 to 15 minutes to arrive at a site depending on distance of drone team from area of interest/impact | 5cm per px for typical photogrammetric drone, Accuracy: 10m | 40 Acres per flight (30 minutes) with typical quadcopter | Drone images and video |
| 25 minutes from arrival at site to deliver initial images of site. | 5cm per px, Accuracy: 10m  | 2,000 Acres per flight (90 minutes) with a typical fixed wing | Drone images and video |
| 2 - 8 hours from demobilization | 1cm per px, Accuracy: 2cm XY, 5cm Z || 2D orthomosaic, 3D point cloud |
| 3 – 20 hours from demobilization | 1cm per px, Accuracy: 2cm XY, 5cm Z || Various types of analysis and CAD drawings |

**Performance Grid**

| Rating | Turnaround time | Resolution/px | Coverage Area | Recommended Mode |
| --- | --- | --- | --- | --- |
| 1 – 2 | 3 working days | 1.5 m - 10 m+ | 100 km2 to1,000 km2 | Satellite |
| 3 – 4 | 2 working days | 30 cm – 1.5 m | 2 km2 to 20 km2 | Fixed wing drone |
| 5 – 6 | 5 hours | 20 cm | 1 km2 | Fixed wing drone |
| 7 – 8 | 60 minutes | 10 cm | 0.32 km2 | Multi-copter drone |
| 9 – 10 | 30 minutes | 5 cm | 0.004 km2 to 0.16 km2 | Multi-copter drone |

# Appendices: An Introduction to Satellite and Drone Technology

# Satellite Imagery

Over 2,200 satellites orbit Earth today, and the space landscape is rapidly changing. Commonly-used satellites are both from commercial to government-sponsored and include WorldView, QuickBird, IKONOS, Landsat, MODIS, AVHRR, TRMM, AirBus Pleiades, and many more.

## Spectral Capabilities, Resolution, and Frequency

_Source_: [https://www.pgc.umn.edu/guides/commercial-imagery/intro-satellite-imagery/](https://www.pgc.umn.edu/guides/commercial-imagery/intro-satellite-imagery/)

Satellites extract information from energy interacting with the Earth&#39;s surface. Remote sensing sensors measure the electromagnetic radiation from reflection, emission, and emission reflection. Light acts as a wave that can be described by its wavelength and frequency, comprising the electromagnetic spectrum. Longer wavelengths contain less energy and shorter wavelengths contain more.

![Electromagnetic Spectrum](/static/guides/satvdronesd4g001.png) 

_Figure 1 The electromagnetic spectrum._

The electromagnetic spectrum is broad and not all wavelengths are equally effective nor significantly interact with surfaces of interest for remote sensing purposes.

Ranges of the electromagnetic spectrum used in remote sensing encompass the following wavelengths:

### Visible

Blue: 0.4 – 0.5 µm, Green: 0.5 – 0.6 µm, and Red: 0.6 – 0.7 µm

### Near-infrared

0.7 – 1.2 µm: Distinguishes green vegetation well

### Mid-infrared

Shortwave infrared (SWIR): 1.2 – 3 µm estimates soil and vegetation moisture well

3 – 8 µm detects high temperature sources well

### Resolution

The resolution of an image refers to the potential detail provided by the imagery. In remote sensing we refer to three types of resolution: spatial, spectral and temporal.

Spatial Resolution refers to the size of the smallest feature that can be detected by a satellite sensor or displayed in a satellite image. It is usually presented as a single value representing the length of one side of a square. For example, a spatial resolution of 250m means that one pixel represents an area 250 by 250 meters on the ground.

Spectral Resolution refers to the ability of a satellite sensor to measure specific wavelengths of the electromagnetic spectrum. The finer the spectral resolution, the narrower the wavelength range for a particular channel or band.

Temporal resolution refers to the time between images. The capability for satellites to provide images of the same geographical area more frequently has increased dramatically since the dawn of the space age.

## Examples of satellite imagery and use cases

![SPOT5 and Terra Satellite Images](/static/guides/satvdronesd4g002.png) 
_Figure 2 Image Classification. Source: C. Kuenzer et al._

### Image Classification
A frequently-used method for information extraction from remote sensing data is to match information classes of to spectral ranges (or a combination of spectral ranges).
Land cover and land use maps, such as those classifying mangroves from SPOT and TerraSAR-X imagery in the Mekong Delta, provide a greater understanding of environments and their processes. |
| --- | --- |

### Risk Assessment

Initiatives like the Mapping Malaria Risk in Africa project (MARA/ARMA) have produced maps of climate suitability for Malaria transmission by characterizing the conditions of the disease necessary for transmission and correlating them with key climatic variables observed from remotely sensed imagery.

### Agriculture

High spatial resolution, false color composites acquired from TTAMRSS (a multispectral, airborne remote sensing system) helped farmers and agricultural consultants in Texas plan different crop and yield management practices.

### Natural Resource Management

AVHRR, MODIS, and SPOT sensors quantitatively measure, for one, the gain and loss of our global forests.

From this data, scientists gauge forest stand areas and estimate valuable forest resources like wood, food, medicine, and absorption of carbon dioxide.

### Coastal and Marine

Sea shelf ecosystems help control our climate and supply food. The National Oceanography Centre (NOC) measured phytoplankton, the microscopic marine plants at the heart of the marine food chain, as a key indicator of the productivity of an ocean area all from space.

### Urban Geography

The conflict in Syria is characterized by violations of human rights and defiance of international law. The Syrian government has demolished neighborhoods suspected of supporting opposition forces.

Scientists used Landsat imagery and its historical archive to monitor and document the destruction of conflict-affected urban environments. Source: A. Marx, Claremont Graduate University

### Emergency Management and Response

Online communities such as OpenStreetMap, Tomnod, CrisisMappers, Virtual Disaster Viewer, and Google MapMaker are utilizing remotely sensed imagery to provide support in disaster preparedness and emergency response.

# Drone Imagery

_Source_: [https://internetofthingsagenda.techtarget.com/definition/drone-photography](https://internetofthingsagenda.techtarget.com/definition/drone-photography)

Drone photography is the capture of still images and video by a remotely-operated or autonomous unmanned aerial vehicle (UAV), also known as an unmanned aircraft system (UAS) or, more commonly, as a drone. Drone photography allows images and audio/video to be captured that might not be otherwise possible for human photographers and videographers and in some cases satellites. That capacity can be enabled by the flight abilities of drones, their small size or their ability to tolerate harsh environments.

UAV drones are equipped with different state of the art technology such as infrared cameras, GPS and laser (consumer, commercial and military UAV). Drones are controlled by remote ground control systems (GSC) and also referred to as a ground cockpit. An unmanned aerial vehicle system has two parts, the drone itself and the control system.

Drone images are taken via drones with cameras or other imaging sensor attached to the drone. Single-frame imagery acquired by drones is essential for creating geospatial products like orthomosaics, digital terrain models, or 3D textured meshes.

## Orthomosaic

_Source_: [https://pro.arcgis.com/en/pro-app/latest/help/data/imagery/generate-an-orthomosaics-using-the-orthomosaic-wizard.htm](https://pro.arcgis.com/en/pro-app/latest/help/data/imagery/generate-an-orthomosaics-using-the-orthomosaic-wizard.htm)

An orthomosaic is a collection of images that have been stitched together, where the geometric distortion has been corrected and the imagery has been color balanced to produce a seamless mosaic dataset.

![](RackMultipart20211120-4-vlfarq_html_229327a60ab78292.png)

Orthomosaic formats:

- Cloud Raster Format
- TIFF Format
- JPEG Format
- JPEG2000 Format
- Meta Raster Format

## Digital elevation and terrain models

_Source_: [https://www.heliguy.com/blogs/posts/drones-and-dems-vs-dtms-vs-dsms](https://www.heliguy.com/blogs/posts/drones-and-dems-vs-dtms-vs-dsms)

Digital Elevation Models, also known as DEMs, are a visual representation of the elevation data for every point captured on your site. Instead of depicting how your site actually appears in person, DEMs generally use colour to indicate differences in elevation - as this image shows.

![](RackMultipart20211120-4-vlfarq_html_9bd9b1f716a72e8b.png)

DEMs are raster datasets, or grids that list the highest vertical point (z) recorded for every horizontal coordinate (x,y). The smaller the grid cells, the more detailed the information is within a DEM data file. So, if you&#39;re looking to model with lots of detail, then small grid spacing (or small cell size) is the one to go for. When visualised, these raster files depict the shape of your site&#39;s terrain, which can be used as a reference for the topography you&#39;re working on.

## Digital Surface Models And Digital Terrain Models

Digital Elevation Models (DEM) are a superset of both Digital Terrain Models (DTM) and Digital Surface Models (DSM). So what is the difference between a DTM and DSM?

DTM: A bare-earth surface model. In other words, featuring purely the terrain and ground.

DSM: The most general form of surface model that includes all acquired points, representing natural and man-made features. A DSM includes the tops of buildings, trees, powerlines and other objects. In essence, it is a canopy model, and only sees the ground when nothing else is above it.

The below graphic is a simple diagram showing the differences between a DTM and DSM. Notice how the DTM (orange line) follows the ground, whereas the DSM (green line) follows the structures on the surface, ie the top of the house and the tree.

![](RackMultipart20211120-4-vlfarq_html_cf769b31e2522d8e.png)

The below example demonstrates this - with the DTM on the left and the identical DSM on the right.

![](RackMultipart20211120-4-vlfarq_html_11abd07c8d887b01.png)

## How Can Drones Help Generate DEMs, DSMs and DTMs?

Drones are a cost-effective, quick and effective data collection tool for DEM, DSM and DTM generation.

Advancements in technology in recent years has led to the development of drones which can conduct photogrammetry and LiDAR missions. Both of these applications are fundamental to creating DEMs, DSMs, and DTMs. Our LiDAR vs Photogrammetry guide presents an in-depth comparison between the two, but in a nutshell:

LiDAR: An acronym of light detection and ranging, LiDAR works by sending pulses of light to the earth&#39;s surface or a feature on it and measuring the time it takes to reflect back. This gives an accurate positioning point of where on the earth&#39;s surface the laser hit and this data can be used to build a 3D point cloud which represents the scanned area&#39;s terrain, topography, and features.

Photogrammetry: The art of capturing high-resolution photographs to recreate a survey area. These images are processed and stitched together using sophisticated software to create realistic, geo-referenced, and measurable 3D models of the real world.

![](RackMultipart20211120-4-vlfarq_html_87ff60103350d526.png)

As the above highlights, there are clear differences between LiDAR and photogrammetry.

Therefore, careful consideration is needed before deploying these drone mapping techniques for DTM, DSM and DEM generation. LiDAR, for instance, is particularly effective for capturing DTM data, compared to photogrammetry - especially in high-vegetation areas. Why? Because by its very nature, LiDAR can penetrate gaps between leaves and branches to reach ground level, therefore obtaining more accurate data of the terrain.

In contrast, vegetation can hinder drone photogrammetry from acquiring an accurate representation of what the ground topography looks like. In essence, it will just capture to top of the features protruding from the earth&#39;s surface.

For more information go to [https://www.heliguy.com/blogs/posts/drones-and-dems-vs-dtms-vs-dsms](https://www.heliguy.com/blogs/posts/drones-and-dems-vs-dtms-vs-dsms)

## Benefits of drone mapping

![](RackMultipart20211120-4-vlfarq_html_18b367b3772b855b.png)

Drone imaging in general is cheaper than Satellite imaging for small areas less than 100Km2 in size. This is because a typical impact area for a community is less than 1km2 and satellite companies do fresh imaging missions for minimum 100Km2 areas. For a quick 5-hectare community or impact area, tasking a satellite to capture 100 Km2 (100 hectares) is very inefficient. The smaller the area to be mapped the more appropriate drones are as the option for mapping.

Drone data is exhaustive in that it is very detailed and can capture in much higher resolutions than other aerial imaging options. Satellite imaging has the highest resolution per pixel of 30cm and drone imaging highest resolution per pixel of 1cm per pixel.

The other advantage of drones is the ability to capture images in difficult to access areas, this can include difficult terrain or underneath cloud cover.

# Terminology

Below is a list of commonly used satellite and drone imagery terms and definitions.

**Radiometric correction** : The process of removing the effects of the atmosphere on the reflectance values of images taken by satellite or airborne sensors.

**Bit depth** : The range of values that a particular raster format can store, based on the formula 2n. An 8-bit depth dataset can store 256 unique values.

**Cloud cover** : The fraction of an image that is obscured by clouds.

**Digital Elevation Model (DEM)**: A digital model or 3D representation of a terrain&#39;s surface.

**Digital Number (DN)**: The original value assigned to a pixel in an image. The DN value will always be within the range determined by the image bit-depth. Thus, an 8-bit depth image will have DNs withing the range of 0-255.

**False color:** A group of color rendering methods used to display images in color which were recorded in the visible or non-visible parts of the electromagnetic spectrum to enhance various ground features.

**Ground Sampling Distance (GSD):** The distance between two consecutive pixel centers measured on the ground. The bigger the value of the image GSD, the lower the spatial resolution of the image and the less visible details.

**Histogram Stretch:** A simple image enhancement technique that attempts to improve the contrast in an image by `stretching&#39; the range of intensity values it contains to span a desired range of values, e.g. the full range of pixel values that the image type concerned allows. Often referred to as contrast stretching or normalization.

**Monoscopic:** Imagery that is obtained using only one viewpoint or sensor.

**Multispectral:** Imagery that captures data at specific frequencies across the electromagnetic spectrum, producing several spectral bands.

**Nadir (off-nadir):** The direction pointing directly below a particular location or sensor, indicating the angle at which an image was obtained.

**Near Infrared bands (NIR):** Bands collected from frequencies along the electromagnetic spectrum closest to visible light. Useful in biological observations and analyses.

**Orbit:** The curved path of a celestial object or spacecraft around a star, planet, or moon.

**Orthorectification:** The process of removing the effects of image perspective (tilt) and relief (terrain) effects for the purpose of creating a planimetrically correct image.

**Panchromatic:** An image with a single band, generally displayed as shades of gray.

**Pixel:** The smallest unit of information (short for &quot;picture element&quot;) in an image or raster map, usually represented as a cell in an array of data values.

**Raster:** A type of digital image composed of individual pixels of various data values or colors.

**Reflectance:** The proportion of incident radiant energy that is reflected by a surface. Reflectance varies according to the wavelengths of the incident radiant energy and the color and composition of the surface.

**Remote sensing:** The acquisition of information about a location on earth from satellite or aerial vehicles, in contrast to on-site observation.

**Spatial resolution:** A term that refers to the size of the smallest possible feature that can be detected of an image.

**Spectral band:** Image data at a specific frequency across the electromagnetic spectrum, including both visible and non-visible light.

**Stereoscopic:** A process by which two photographs of the same object taken at slightly different angles are viewed together, creating an impression of depth.

**Swath:** The area imaged on the surface of the earth as a sensor passes over.

**Temporal resolution:** A term that refers to precision and measurement with respect to time and commonly describes how often an orbiting sensor can return to the same location on the earth.

**Thermal bands:** Image data usually collected from the thermal infrared region of the electromagnetic spectrum and measures the radiance emitted by the surface of the target. This is in contrast to measuring the reflectance of target features in optical imaging.

**True color (rgb):** An image that offers a natural color rendition, appearing to a human observer the same way as if this observer were to directly view the object.

**Sensor:** An object whose purpose is to detect events or changes in its environment. In remote sensing. In remote sensing, the term usually refers to the instrument collecting imagery from either airborne or spaceborn vehicles.
